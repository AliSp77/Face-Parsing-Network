[Face Parsing]

* Generative Face Completion [Y. Li, 2017] https://arxiv.org/abs/1704.05838 을 위해 구현한 네트워크
* 위 Generative Face Completion에 쓰인 'Object contour detection with a fully convolutional encoder-decoder network [J. Yang, 2016]' 를 PyTorch 기반으로 구현한 것
* 사용 DB는 CelebA-HQ 데이터베이스로, 30,000개의 이미지를 포함하며 16개의 얼굴 부위가 세그멘테이션 되어있음
 (GT는 배경은 0, 각 부위 클래스는 1~16의 숫사로 레이블링 되어있음, https://drive.google.com/open?id=1oR4Ja2rO9k66zV8JTLtdOVrCnW7zH0xW에 30,000개의 이미지와 GT 이미지 업로드해두었음)

* 30,000개의 이미지를 모두 가지고 학습한 모델 : https://drive.google.com/open?id=1e38G_bTvsktDkgZRyG-V7Yk6gR9yO7u3
  CelebA-HQ를 모두 가지고 학습한 Parsing 네트워크로 다른 얼굴 DB(LFW, CelebA)에서 Ground-Truth를 만들기 위해 사용한 모델임




<구성 파일>
seg_config.yaml : config 파일. DB 경로 및 이미지 크기 등은 여기서 수정함
seg_train.py : 학습 시작 코드
seg_trainer.py : 학습에 필요한 네트워크 및 loss 처리를 하는 코드
network_seg_contour.py : parsing 네트워크
utils/* : 로그 출력 및 데이터셋 구조체 관련 코드

<실행방법>
python seg_train.py

<DB 경로 및 설정>
seg_config.yaml에서 설정 변경 가능
* gpu_ids, num_workers : GPU 병렬처리 옵션 설정 
* all_data_path : 학습에 사용할 얼굴 이미지들이 들어있는 디렉터리
* gt_path : 위 학습에 사용할 얼굴 이미지의 parsing GT들이 들어있는 디렉터리
* test_data_path : 일정 iteration마다 loss 확인을 위해 사용할 test/validation 얼굴 이미지들이 들어있는 디렉터리
* output_test_path : 일정 iteration마다 test/validation의 세그멘테이션된 이미지 결과들을 저장 및 확인할 디렉터리
* image_shape : input 이미지 크기 (기본적으로 128*128으로 설정해두었음)
* resume : 이전에 저장된 checkpoint 파일 불러오기
* resume_iter : resume을 이용해 저장된 checkpoint를 불러올 때, 몇 번째 iteration의 (checkpoint 파일명에 쓰여있음) 파일을 불러올 것인가
* batch_size : Batch 크기 (기본적으로 24로 설정해두었음)
* random_crop : DB의 이미지가 128*128보다 크면 crop을 수행하는데, 랜덤하게 crop할것인가의 여부 (기본적으로 가운데서부터 crop 수행)
* niter : iteration 횟수
* 그 외의 항목은 크게 중요하지 않거나 안 쓰이는 항목들
